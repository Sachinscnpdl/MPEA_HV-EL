{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10858e5f",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d5e914",
   "metadata": {},
   "source": [
    "## 1.1 Import Relevant Libraries and .CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4f773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurization(df):\n",
    "    df = stc.featurize_dataframe(df, \"Alloys\",ignore_errors=True,return_errors=True)\n",
    "    df = ef.featurize_dataframe(df, \"composition\",ignore_errors=True,return_errors=True)   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4737dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def properties_calculation(dataframe):\n",
    "    # Input featurs calculation\n",
    "    df = dataframe\n",
    "    properties = []\n",
    "    for number in range(len(df['Alloys'])):\n",
    "\n",
    "        mpea = df['composition'][number]\n",
    "        #print(Composition(mpea).as_dict().keys())\n",
    "        element = list(Composition(mpea).as_dict().keys()) # List element present in Alloys ['Al', 'Cr', 'Fe', 'Ni', 'Mo']\n",
    "        fraction_composition = list(Composition(mpea).as_dict().values()) # List Fraction composition of corresponding element in an Alloy eg. [1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "        total_mole = sum(fraction_composition) # Sum of elemental composition\n",
    "\n",
    "        atomic_number = []\n",
    "        bulk_modulus = []\n",
    "        shear_modulus = []\n",
    "        molar_heat = []\n",
    "        thermal_conductivity = []\n",
    "        mole_fraction = []\n",
    "        X_i = []\n",
    "        r_i = []\n",
    "        Tm_i = []\n",
    "        VEC_i= []\n",
    "        R = 8.314\n",
    "\n",
    "        for i in element:\n",
    "\n",
    "            atomic_number.append(Element(i).Z)\n",
    "            #molar_heat.append(Cp_dict[i])\n",
    "\n",
    "            bulk_b =Element(i).bulk_modulus\n",
    "\n",
    "            if type(bulk_b) == type(None):\n",
    "                for j in bulk_modulus_b: bulk_b = (bulk_modulus_b.get(j))       \n",
    "\n",
    "            bulk_modulus.append(bulk_b)\n",
    "\n",
    "            #print(bulk_modulus)\n",
    "\n",
    "            shear_g = (Element(i).rigidity_modulus)\n",
    "            if type(shear_g) == type(None):\n",
    "                for s in shear_modulus_g: shear_g = ((shear_modulus_g.get(s)))\n",
    "            shear_modulus.append(shear_g)\n",
    "\n",
    "            thermal_conductivity.append(Element(i).thermal_conductivity)\n",
    "            mole_fraction.append(Composition(mpea).get_atomic_fraction(i)) # Calculates mole fraction of mpea using \"Composition\" functions\n",
    "\n",
    "            X_i.append(Element(i).X) # Calculate individual electronegativity using \"Element\" function\n",
    "\n",
    "            r_i.append(Element(i).atomic_radius) if Element(i).atomic_radius_calculated == None else r_i.append(Element(i).atomic_radius_calculated) # There are two functions present in Element␣class of pymatgen, so here checking using if conditional in both functions␣to not miss any value\n",
    "            Tm_i.append(Element(i).melting_point) # Calculating melting point of every element using \"Element\" class and function\n",
    "            try: VEC_i.append(DemlData().get_elemental_property(Element(i),\"valence\")) # VEC is also present in 2 locations in matminer, first is the␣function \"DemlData()\"\n",
    "            except KeyError:\n",
    "                if i in VEC_elements: VEC_i.append(float(VEC_elements.get(i))) #In case data is not present in \"DemlData()\" function, there is a csv file␣inside matminer opened earlier as \"elem_prop_data\" in the very first cell\n",
    "\n",
    "        # Average Atomic Number\n",
    "        AN = sum(np.multiply(mole_fraction, atomic_number))\n",
    "\n",
    "        # Average Molar Heat coefficient\n",
    "        #Cp_bar = sum(np.multiply(mole_fraction, molar_heat))    \n",
    "        #print(Cp_bar)\n",
    "\n",
    "        #term_Cp = (1-np.divide(molar_heat, Cp_bar))**2\n",
    "        #del_Cp = sum(np.multiply(mole_fraction, term_Cp))**0.5 \n",
    "\n",
    "        # Thermal Conductivity\n",
    "        k = sum(np.multiply(mole_fraction, thermal_conductivity))\n",
    "\n",
    "        # Bulk Modolus\n",
    "        bulk = sum(np.multiply(mole_fraction, bulk_modulus)) # Bulk modulus of 'Zr' not present\n",
    "        \n",
    "        # Bulk modolus asymmetry\n",
    "        term_bulk = (1-np.divide(bulk_modulus, bulk))**2\n",
    "        del_bulk = sum(np.multiply(mole_fraction, term_bulk))**0.5         \n",
    "\n",
    "        # Shear Modolus\n",
    "        shear= sum(np.multiply(mole_fraction, shear_modulus))\n",
    "        \n",
    "        # Shear modolus asymmetry\n",
    "        term_shear = (1-np.divide(shear_modulus, shear))**2\n",
    "        del_shear = sum(np.multiply(mole_fraction, term_shear))**0.5         \n",
    "\n",
    "        # Calculation of Atomic Radius Difference (del)\n",
    "\n",
    "        r_bar = sum(np.multiply(mole_fraction, r_i))\n",
    "        term = (1-np.divide(r_i, r_bar))**2\n",
    "        atomic_size_difference = sum(np.multiply(mole_fraction, term))**0.5 \n",
    "        #print(number,element,mole_fraction,r_i,r_bar,term,atomic_size_difference)\n",
    "\n",
    "\n",
    "        # Electronegativity (del_X)\n",
    "        X_bar = sum(np.multiply(mole_fraction, X_i))\n",
    "        del_Chi = (sum(np.multiply(mole_fraction, (np.subtract(X_i,X_bar))**2)))**0.5\n",
    "        #term_X = (1-np.divide(X_i, X_bar))**2\n",
    "        #del_Chi = (sum(np.multiply(mole_fraction, term_X)))**0.5 \n",
    "\n",
    "        # Difference Melting Temperature\n",
    "        T_bar = sum(np.multiply(mole_fraction, Tm_i))\n",
    "        del_Tm =(sum(np.multiply(mole_fraction, (np.subtract(Tm_i,T_bar))**2)))**0.5\n",
    "\n",
    "        # Average Melting Temperature\n",
    "        Tm = sum(np.multiply(mole_fraction, Tm_i))    \n",
    "\n",
    "        # Valence Electron Concentration\n",
    "        VEC = sum(np.multiply(mole_fraction, VEC_i))\n",
    "\n",
    "        # Entropy of mixing\n",
    "        del_Smix = -WenAlloys().compute_configuration_entropy(mole_fraction)*1000 #WenAlloys class imported from matminer library\n",
    "\n",
    "\n",
    "\n",
    "        HEA = element\n",
    "        #print(len(mole_fraction), len(HEA))\n",
    "\n",
    "\n",
    "        # Enthalpy of mixing\n",
    "        AB = []\n",
    "        C_i_C_j = []\n",
    "        del_Hab = []\n",
    "        for item in range(len(HEA)):\n",
    "            for jitem in range(item, len(HEA)-1):\n",
    "                AB.append(HEA[item] + HEA[jitem+1])\n",
    "                C_i_C_j.append(mole_fraction[item]*mole_fraction[jitem+1])\n",
    "                del_Hab.append(round(Miedema().deltaH_chem([HEA[item], HEA[jitem+1]], [0.5, 0.5], 'ss'),3)) # Calculating binary entropy of mixing at 0.5-0.5␣ (equal) composition using Miedema class of \"matminer\" library\n",
    "\n",
    "        omega = np.multiply(del_Hab, 4)\n",
    "        del_Hmix = sum(np.multiply(omega, C_i_C_j))\n",
    "\n",
    "        # Geometrical parameters\n",
    "        lemda = np.divide(del_Smix, (atomic_size_difference)**2)\n",
    "        #print(number,del_Smix,atomic_size_difference, lemda)\n",
    "\n",
    "        parameter = Tm*del_Smix/abs(del_Hmix) \n",
    "        #print(number,\"lemda, parameter\", lemda, parameter)\n",
    "\n",
    "\n",
    "        properties.append([len(element), \" \".join(element), \" \".join(list(map(str, fraction_composition))),total_mole, round(sum(mole_fraction),1), atomic_size_difference, round(del_Chi, 4),del_Tm, Tm, VEC, AN, k, bulk,del_bulk,shear,del_shear, round(del_Smix, 4),round(lemda,4), round(del_Hmix, 4),round(parameter,4)])\n",
    "    prop_data = pd.DataFrame(properties, columns=['No of Components','Component','Moles of individual Components', 'Total Moles', 'Sum of individual MoleFractions', '$\\delta$', 'Δ$\\chi$', 'ΔTm','Tm(K)', 'VEC', 'AN', 'K','B', 'ΔB','G', 'ΔG','ΔSmix','$\\lambda$', 'ΔHmix','$\\Omega$'])\n",
    "    df = pd.concat([df, prop_data], axis = 1)\n",
    "    \n",
    "    df_input_target = df.iloc[:,[-20,-15,-14,-13,-12,-11,-10, -9, -8, -7, -6,-5, -4, -3, -2, -1,1]]\n",
    "    \n",
    "    return(df,df_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_new(y_test,ideal_weighted_ensemble_prediction):\n",
    "    import sklearn.metrics\n",
    "    from sklearn.metrics import r2_score\n",
    "\n",
    "    ideal_weighted_ensemble_prediction\n",
    "\n",
    "\n",
    "    #Predict on test data\n",
    "    y_test\n",
    "    a=0.2 # Percentage error range\n",
    "    r2_mean = r2_score(y_test,ideal_weighted_ensemble_prediction)\n",
    "    plt.figure(figsize=(4,4.5),dpi=600)\n",
    "\n",
    "    # plot x=y line \n",
    "    x_line = np.linspace(0, 1200, 1000)\n",
    "\n",
    "    sns.lineplot(x=x_line, y=x_line,color='black',lw=0.75)\n",
    "\n",
    "    print('Test R2 score: ', r2_mean)\n",
    "\n",
    "    test_r2 = sns.regplot(x=y_test,y=ideal_weighted_ensemble_prediction,ci=None,scatter_kws=dict(s=8,color='r'),fit_reg=False)\n",
    "    test_r2.set(title='Test $R^2 = $' +str(round(r2_mean,2))+' with 20% Error region')\n",
    "    test_r2.set_xlabel(\"Real Test Targets\", fontsize = 18)\n",
    "    test_r2.set_ylabel(\"Predicted Test Value\", fontsize = 18)\n",
    "\n",
    "    Y1 = x_line*(1+a)\n",
    "    Y2 = x_line*(1-a)\n",
    "\n",
    "    sns.lineplot(x=x_line,y=Y1,lw=0.5,color='b',alpha=.2)\n",
    "    sns.lineplot(x=x_line,y=Y2,lw=0.5,color='b',alpha=.2)\n",
    "\n",
    "    plt.fill_between(x_line, Y1,x_line,color='b',alpha=.2)\n",
    "    plt.fill_between(x_line, Y2,x_line,color='b',alpha=.2)\n",
    "\n",
    "    #test_r2.figure.savefig('plots\\\\r2_test_hardness.pdf',dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35289e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_element_number(df):\n",
    "    # Add a column of \"Number of component\" & \"component\" in each alloy system\n",
    "    prop = []\n",
    "    for number in range(len(df['Alloys'])):\n",
    "        mpea = df['composition'][number]\n",
    "        element = list(Composition(mpea).as_dict().keys()) # List element present in Alloys ['Al', 'Cr', 'Fe', 'Ni', 'Mo']\n",
    "        prop.append([len(element), \" \".join(element)])\n",
    "\n",
    "        prop_data = pd.DataFrame(prop, columns=['No of Components', 'Component'])\n",
    "    df = pd.concat([df, prop_data], axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13403e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def element_number(df, fig_title='Hardness', fig_name='element_number_hardness'):\n",
    "    import os\n",
    "    import matplotlib\n",
    "    import matplotlib.ticker as tck\n",
    "    #matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(3.5,3.5),dpi=600)\n",
    "    df['No of Components'].value_counts().plot(ax=ax, use_index=True, kind = 'bar')\n",
    "\n",
    "    ax.tick_params(axis='x', labelrotation=0)\n",
    "    plt.rcParams['font.size'] = '20'\n",
    "\n",
    "    #plt.rcParams.update({'font.size': 20})\n",
    "    plt.xlabel('Number of element in MPEA', fontsize=18)\n",
    "    plt.ylabel('Count', fontsize=20)\n",
    "    plt.title(fig_title,fontsize=20)\n",
    "\n",
    "    ax.yaxis.set_minor_locator(tck.AutoMinorLocator(2))\n",
    "    ax.tick_params(bottom=True, left=True)\n",
    "    #plt.tick_params(axis='both', which='both', length=3, width=2,color='black')\n",
    "    \n",
    "    plot_fig_loc = fig_name\n",
    "    plt.savefig(plot_fig_loc,dpi=1200, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c730ac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fab_type(df, fig_title='Hardness/ Elongation', fig_name='fab_type_hardness'):   \n",
    "    import matplotlib.ticker as tck\n",
    "    fig, ax = plt.subplots(figsize=(6,5),dpi=600)\n",
    "\n",
    "    \n",
    "    df['Fabrication_type'].replace(to_replace=[\"CAST\"],\n",
    "           value=\"CAT-A\", inplace= True)\n",
    "    \n",
    "    df['Fabrication_type'].fillna(\"Unknown\",inplace=True)\n",
    "    \n",
    "    df['Fabrication_type'].replace(to_replace=[\"OTHER\",\"Unknown\",\"WROUGHT\"],\n",
    "           value=\"CAT-B\", inplace= True)    \n",
    "\n",
    "    df['Fabrication_type'].replace(to_replace=[\"POWDER\"],\n",
    "           value=\"CAT-C\", inplace= True) \n",
    "    \n",
    "    df['Fabrication_type'].replace(to_replace=[\"ANNEAL\"],\n",
    "           value=\"CAT-D\", inplace= True) \n",
    "\n",
    "    df['Fabrication_type'].value_counts(dropna=False).plot(ax=ax, use_index=True, kind = 'bar')    \n",
    "    \n",
    "    ax.tick_params(axis='x', labelrotation=20)\n",
    "    #plt.xlabel('Fabrication route', fontsize=10)\n",
    "    plt.ylabel('Count', fontsize=18)\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "    #ax.set_xticklabels( ('Cast', 'Unknown','Powder','Anneal','Other', 'Wrought') )\n",
    "\n",
    "    plt.tick_params(axis='both', which='both', length=3, width=1,color='black')\n",
    "    ax.yaxis.set_minor_locator(tck.AutoMinorLocator(2))\n",
    "    #plt.ylim(0, 620)\n",
    "    plt.title('Fabrication Processes ('+fig_title+')')\n",
    "    plot_fig_loc = fig_name\n",
    "    plt.savefig(plot_fig_loc,dpi=1200, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a6a877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phase_rearrange(df):\n",
    "    df['Phase'].replace(to_replace=[\"FCC+Sec.\", \"FCC+B2+Laves\",  \"FCC+B2+Sec.\", \"FCC + Im\", \"FCC+L12+Sec\", \"FCC+B2\",\"FCC+Laves\", \"FCC+L12+Sec.\", \"FCC+HCP+Sec.\" ],\n",
    "           value=\"FCC+Im\", inplace= True)\n",
    "\n",
    "    df['Phase'].replace(to_replace=[\"BCC+Sec.\",\"B2+BCC\", \"BCC+B2\",\"BCC+B2+Laves\", \"BCC+Laves+Sec.\", \"BCC+Laves\",  \"BCC+B2+Sec.\", \"BCC + Im\", \"BCC + lm\",\"BCC+B2+L12\",\"BCC+BCC+Sec.\", \"BCC+Laves+Sec.\", \"BCC +Im\"],\n",
    "               value=\"BCC+Im\", inplace= True)\n",
    "\n",
    "    df['Phase'].replace(to_replace=[\"FCC+BCC\", \"FCC + BCC\", \"FCC+BCC+BCC\", \"BCC+FCC\"],\n",
    "               value=\"FCC+BCC\", inplace= True)\n",
    "\n",
    "    df['Phase'].replace(to_replace=[\"FCC+BCC+Sec.\",\"FCC+BCC+B2\",\"FCC+BCC+B2+Sec.\", \"FCC + BCC + Im\", \"FCC + BCC + B2\"],\n",
    "               value=\"FCC+BCC+Im\", inplace= True)\n",
    "\n",
    "    df['Phase'].replace(to_replace=[\"B2+Sec.\",\"B2\",\"Im\", \"FCC + BCC + Im\"],\n",
    "               value=\"FCC+BCC+Im\", inplace= True)\n",
    "\n",
    "    df['Phase'].replace(to_replace=[\"FCC+FCC\", \"FCC\", \"FCC+HCP\", \"FCC + HCP\"],\n",
    "               value=\"FCC\", inplace= True)\n",
    "\n",
    "    df['Phase'].replace(to_replace=[\"BCC+HCP\", \"BCC\",\"BCC+BCC\"],\n",
    "               value=\"BCC\", inplace= True)\n",
    "    \n",
    "    df['Phase'].replace(to_replace=[\"L12+B2\", \"B2+L12\", \"\"],\n",
    "               value=\"Im\", inplace= True)\n",
    "\n",
    "    df['Phase'].fillna(\"Unknown\",inplace=True)\n",
    "\n",
    "    df['Phase'].replace(to_replace=[\"Unknown\",0, \"Other\"],\n",
    "               value=\"Unknown\", inplace= True)\n",
    "    \n",
    "    df['Phase'].value_counts(dropna=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cdaeb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phase_plot(df, fig_title='Hardness/ Elongation', fig_name='fab_type_hardness'):\n",
    "    import seaborn as sns\n",
    "    import matplotlib.ticker as tck\n",
    "    sns.set(style=\"ticks\", color_codes=True,font_scale=2.25)\n",
    "    ax = plt.figure(figsize=(16,5),dpi=600)\n",
    "    plt.tick_params(axis='x', labelrotation=30)\n",
    "    sns.set(style='white',font_scale=2)\n",
    "    sns.set_style('white', {'axes.linewidth': 0.5})\n",
    "    plt.rcParams['xtick.major.size'] = 50\n",
    "    plt.rcParams['xtick.bottom'] = True\n",
    "    plt.rcParams['ytick.left'] = True\n",
    "\n",
    "\n",
    "    sns.boxplot(x=df['Phase'],y=df[fig_title], width= 0.4)\n",
    "    plt.xticks(rotation=10)\n",
    "\n",
    "    plt.rcParams.update({'font.size': 24})\n",
    "    plt.tick_params(axis='both', which='both', length=3, width=1,color='black')\n",
    "    ax.gca().yaxis.set_minor_locator(tck.AutoMinorLocator(2))\n",
    "    \n",
    "    plot_fig_loc = fig_name\n",
    "    plt.savefig(plot_fig_loc,dpi=1200, bbox_inches='tight')\n",
    "    #plt.savefig('element_phase_hardness.png',dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d935427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def element_occurrence(df,limit_value=8, fig_title='Hardness/ Elongation', fig_name='element_occurrence'):\n",
    "    \n",
    "    df = df.loc[:, (df != 0).any(axis=0)]\n",
    "\n",
    "    cols = list(df.columns.values)    #Make a list of all of the columns in the df\n",
    "    set = df.astype(bool).sum(axis=0) # Extract the occurance of each element in the alloys\n",
    "\n",
    "    element_df = set.to_frame()      # Convert extracted the occurance of each element in dataframe\n",
    "\n",
    "    element_occurancy = element_df[limit_value:-3]\n",
    "    element_occurancy.columns =['Occurrence']\n",
    "\n",
    "    # Plotted shorted frequency of occurance\n",
    "    #plt.figure(figsize=(30,6),dpi=600)\n",
    "    element_occurancy.sort_values(\"Occurrence\").plot(y='Occurrence', use_index=True, kind = 'bar') \n",
    "    plt.tick_params(axis='x', labelrotation=90)\n",
    "    plt.ylabel('Count', fontsize=20)\n",
    "\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    plt.tick_params(axis='both', which='both', length=3, width=1,color='black')\n",
    "    plt.gca().yaxis.set_minor_locator(MultipleLocator(50))\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = [14,6]\n",
    "\n",
    "    \n",
    "    print(element_occurancy.sort_values(\"Occurrence\"))\n",
    "    print(\"\\n The total number of datasets used for ML model is\", element_df.iat[0,0])\n",
    "    plt.xticks(rotation=10)\n",
    "    #plt.savefig('element_occurance_hardness.png',dpi=1200)\n",
    "    plot_fig_loc = fig_name\n",
    "    plt.savefig(plot_fig_loc,dpi=1200, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c9e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_elimination(df):\n",
    "    # Eliminate dataset consisting less than 3 and greater than 8 element & alloys with fewer element occurance\n",
    "\n",
    "    total_datasets = len(df.index)\n",
    "\n",
    "    df = df[~df.Component.str.contains('Au')]\n",
    "    df = df[~df.Component.str.contains('Nd')]\n",
    "    df = df[~df.Component.str.contains('Ag')]\n",
    "    #df = df[~df.Component.str.contains('Sc')]\n",
    "    df = df[~df.Component.str.contains('Li')]\n",
    "    df = df[~df.Component.str.contains('Mg')]\n",
    "\n",
    "    df = df[~df.Component.str.contains('Re')]\n",
    "    df = df[~df.Component.str.contains('Y')]\n",
    "    #df = df[~df.Component.str.contains('Pd')]\n",
    "\n",
    "\n",
    "    # Reset the index after deleting the rows.\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    datasets_element = len(df.index)\n",
    "    data_eliminated_by_element = total_datasets - datasets_element\n",
    "\n",
    "    # Eliminate alloys with less than 3 elements and more than 8 elements\n",
    "    df = df[(df['No of Components'] > 2) & (df['No of Components'] < 8)]\n",
    "\n",
    "    # Reset the index after deleting the rows.\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    print(\"Total Datasets\", total_datasets)\n",
    "    print(\"Datasets eleminated due to fewer number of element occurance:\", data_eliminated_by_element)\n",
    "    print(\"Datasets eleminated due to number of element in alloy:\", datasets_element -len(df.index))\n",
    "    print(\"Total Datasets for the hardness prediction\", len(df.index))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29cdf687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fab_encoding(df,path='hardness_model_files\\\\'):\n",
    "    df.Fabrication_type = df.Fabrication_type.fillna('OTHER')\n",
    "    df.Fabrication_type = df.Fabrication_type.replace(to_replace=[\"WROUGHT\"],\n",
    "               value=\"OTHER\")\n",
    "    \n",
    "    print(df['Fabrication_type'].value_counts(dropna=False))\n",
    "    \n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    # creating one hot encoder object \n",
    "    enc = OneHotEncoder()\n",
    "    fab_type = pd.DataFrame(enc.fit_transform(df[['Fabrication_type']]).toarray())\n",
    "    \n",
    "    import pickle\n",
    "    pickle.dump(enc, open(path+'fab_encoding.pkl','wb'))\n",
    "    \n",
    "    return(fab_type, enc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cd7eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_target(datasets, fab_type, input_name):\n",
    "    inputs = np.column_stack((fab_type,datasets))\n",
    "    inputs = inputs.astype(float)\n",
    "\n",
    "    df_all = pd.DataFrame(inputs, columns = ['Fab_1','Fab_2','Fab_3','Fab_4','No of Components']+input_name+[\"HV\"])\n",
    "\n",
    "    df_inputs = df_all.drop(['HV'], axis=1)\n",
    "    df_targets = df_all['HV']\n",
    "    return (df_all, df_inputs, df_targets)\n",
    "    \n",
    "def train_test_split(datasets, fab_type, input_name):\n",
    "    \n",
    "    df_all, df_inputs, df_targets = input_target(datasets, fab_type, input_name)\n",
    "    # Split dataset in train-test\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_inputs, df_targets, test_size=0.1, random_state=42)\n",
    "\n",
    "    #X_train, X_test,  = train_test_split(df_inputs, test_size=0.1, random_state=0)\n",
    "\n",
    "    X_train_no_fab = X_train.drop(['Fab_1', 'Fab_2', 'Fab_3', 'Fab_4','No of Components'], axis=1)\n",
    "    X_train_fab = X_train.loc[:,['Fab_1', 'Fab_2', 'Fab_3', 'Fab_4']]\n",
    "\n",
    "    X_test_no_fab = X_test.drop(['Fab_1', 'Fab_2', 'Fab_3', 'Fab_4','No of Components'], axis=1)\n",
    "    X_test_fab = X_test.loc[:,['Fab_1', 'Fab_2', 'Fab_3', 'Fab_4']]\n",
    "    \n",
    "    n_component = X_train.loc[:,['No of Components']]\n",
    "\n",
    "    input_df = pd.concat([X_train_no_fab, y_train], axis=1)\n",
    "    \n",
    "    return(X_train_no_fab, X_train_fab, X_test_no_fab, X_test_fab,y_train, y_test,n_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9093e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def data_distribution(data,text,test_annotate=0,test_annotate2=0,limit=1200,distance=0.001,ylabel = 'Hardness (HV)', title=\"Hardness Distribution\",plot_path=\"plots\\\\hardness\\\\\"):\n",
    "    \n",
    "    import seaborn as sns\n",
    "    import matplotlib.ticker as tck\n",
    "    \n",
    "    mean=data.mean()\n",
    "    median=np.median(data)\n",
    "    ten_per = np.percentile(data, 10)\n",
    "    ninety_per = np.percentile(data, 90)\n",
    "    print(text,'\\n Mean: ',mean,'\\n Median: ',median, '\\n 10 Percentile:',ten_per, '\\n 90 Percentile:', ninety_per)\n",
    "    print(\"Number of \",text, data.shape[0])\n",
    "    print(\"----------------------\")\n",
    "    sns.set(style=\"ticks\", color_codes=True,font_scale=1.15)\n",
    "    \n",
    "    fig , ax = plt.subplots(figsize=(5,5), dpi=400)\n",
    "    sns.kdeplot( y=data, color=\"dodgerblue\",lw=0.5, shade=True, bw_adjust=1)\n",
    "    \n",
    "    # Plot Mean and Median\n",
    "    plt.plot(distance,mean, marker=\"o\", markersize=6, markeredgecolor=\"red\", markerfacecolor=\"red\", label=\"Mean\",linestyle = 'None')\n",
    "    plt.plot(distance,median, marker=\"^\", markersize=6, markeredgecolor=\"blue\", markerfacecolor=\"blue\", label=\"Median\",linestyle = 'None')\n",
    "    \n",
    "    # Plot 10 and 90 percentile\n",
    "    plt.plot(distance,ten_per, marker=\"o\", markersize=5, markeredgecolor=\"red\", label=\"10% Percentile\",linestyle = 'None')\n",
    "    plt.plot(distance,ninety_per, marker=\"o\", markersize=5, markeredgecolor=\"blue\", label=\"90% Percentile\",linestyle = 'None')\n",
    "\n",
    "    x_values = [distance, distance]\n",
    "    y_values = [ten_per, ninety_per]\n",
    "    plt.plot(x_values, y_values, 'green', linestyle=\"-\")\n",
    "    \n",
    "    # Annotations\n",
    "    plt.text(distance*1.06, ten_per-test_annotate, str(int(ten_per))+ \"(10%)\", horizontalalignment='left', size='medium', color='black')\n",
    "    plt.text(distance*1.06, ninety_per+test_annotate2, str(int(ninety_per))+ \"(90%)\", horizontalalignment='left', size='medium', color='black')\n",
    "\n",
    "    #plt.gca().axes.get_xaxis().set_visible(False) # Remove x-axis lable\n",
    "    \n",
    "    plt.ylim(0, limit)\n",
    "    ax.yaxis.set_minor_locator(tck.AutoMinorLocator(2))\n",
    "    \n",
    "    plt.title(title+str(text)+\" (\"+str(data.shape[0])+\" data)\")\n",
    "    plt.ylabel(ylabel)\n",
    "    \n",
    "    # Crop shaded region above max and below min value\n",
    "    plt.axhspan(0,min(data), color='white')\n",
    "    plt.axhspan(max(data),limit, color='white')\n",
    "\n",
    "    plt.legend(frameon=False,loc='upper right')\n",
    "    plt.savefig(plot_path+str(text)+ylabel+'_split.png',dpi=1200, bbox_inches='tight')\n",
    "    #plt.legend(frameon=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354c5219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_data(X_train_no_fab):\n",
    "    # Standarize the input features\n",
    "    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "    scaler = StandardScaler()\n",
    "    robust = RobustScaler()\n",
    "    #robust_X_train = robust.fit_transform(X_train_no_fab)\n",
    "    #scaler.fit(X_train_no_fab)\n",
    "    std_X_train = scaler.fit_transform(X_train_no_fab)\n",
    "\n",
    "    std_df = pd.DataFrame(data=std_X_train, columns=input_name)\n",
    "    #std_df['Hardness (HV)'] = y_train\n",
    "    \n",
    "    return(scaler, std_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3eeee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(std_train_df,name,prop='HV'):\n",
    "    import seaborn as sns\n",
    "    sns.set(style=\"ticks\", color_codes=True,font_scale=2.2)\n",
    "    plt.figure(figsize=(22,12))\n",
    "    cmap = sns.diverging_palette(133,10,s=80, l=55, n=9, as_cmap=True)\n",
    "    cor_train = std_train_df.corr()\n",
    "    sns.heatmap(cor_train, annot=True, fmt='.2f',cmap=cmap) #\n",
    "\n",
    "    plt.savefig(name+'_pcc_all.pdf',dpi=1200)\n",
    "    plt.show()\n",
    "\n",
    "def pcc_fs(std_df,y_train,input_pcc,name,prop='HV'):\n",
    "    \n",
    "    std_all_feature = np.column_stack((std_df,y_train))\n",
    "    std_train_df=pd.DataFrame(data=std_all_feature, columns=input_name+[prop])\n",
    "    heatmap(std_train_df,name)\n",
    "    \n",
    "    X_train_pcc = std_train_df.loc[:,input_pcc+[prop]]\n",
    "    import seaborn as sns\n",
    "    plt.figure(figsize=(13,6))\n",
    "    cor_mid = X_train_pcc.corr()\n",
    "    sns.heatmap(cor_mid, annot=True, cmap= plt.cm.CMRmap_r,fmt='.2f')\n",
    "    plt.savefig(name+'_pcc_fs.png',dpi=1200,bbox_inches='tight')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vif_value(datasets):\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    vif = pd.DataFrame()\n",
    "    vif['VIF Factor'] = [variance_inflation_factor(datasets.values,i) for i in range(datasets.shape[1])]\n",
    "\n",
    "    vif['features'] = datasets.columns\n",
    "\n",
    "    return(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a128a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairplot(std_df_pcc,n_component,name, plot_loc):\n",
    "    import seaborn as sns\n",
    "    df_pp = std_df_pcc.astype(float)\n",
    " \n",
    "    #print(\"n_component:\", n_component.value_counts(dropna=False))    \n",
    "    n_component['No of Components'].replace(to_replace=[2,3,4],\n",
    "        value=\"MEA\", inplace= True)\n",
    "    \n",
    "    n_component['No of Components'].replace(to_replace=[5, 6, 7,8,9],\n",
    "        value=\"HEA\", inplace= True)\n",
    "    \n",
    "    \n",
    "    #print(\"n_component:\", n_component.value_counts(dropna=False))\n",
    "\n",
    "    df_pp[' '] = n_component\n",
    "    #print(\"....\")\n",
    "\n",
    "    #print(\"df_pp\",df_pp[' '].value_counts(dropna=False))\n",
    "\n",
    "    sns.set(style=\"ticks\", color_codes=True,font_scale=2, palette= 'dark')\n",
    "    \n",
    "    g = sns.pairplot(df_pp, hue=' ', markers = ['o', \"*\"],plot_kws={'alpha':0.8,\"s\": 10,'edgecolor': 'none'})\n",
    " \n",
    "    g.fig.set_size_inches(19,11)\n",
    "    plt.title(name, y=-1.25,x=-3,fontsize=36)\n",
    "\n",
    "    plt.savefig(plot_loc+name+'_pc_pairplot.pdf',dpi=1200,bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac0b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_fs(std_df,name, title=\"a) Hardness PCA-1\"):\n",
    "    # Plot PCA graph\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA()\n",
    "    sns.set(style=\"ticks\", color_codes=True,font_scale=2)\n",
    "    principalComponents = pca.fit_transform(std_df)\n",
    "    plt.figure(figsize=(8,6.5))\n",
    "    #plt.figure()\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Principal Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title(title+ ' : Explained Variance ')\n",
    "    plt.grid(alpha=0.5)\n",
    "\n",
    "    x=[1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
    "    plt.xlim(0, 14)\n",
    "    values = range(len(x))\n",
    "    plt.xticks(values, x)\n",
    "\n",
    "    plt.rcParams.update({'font.size': 30})\n",
    "    plt.tick_params(axis='both', which='both', length=3, width=1,color='black')\n",
    "    plt.gca().yaxis.set_minor_locator(MultipleLocator(50))\n",
    "    sns.set(style=\"ticks\", color_codes=True,font_scale=2)\n",
    "\n",
    "    plt.savefig(name+'_fs.pdf',dpi=1200)\n",
    "    plt.show()\n",
    "    \n",
    "    # Principal components to capture 0.9 variance in data\n",
    "    pca_1 = PCA(0.9)\n",
    "    df_pca = pca_1.fit_transform(std_df)\n",
    "    \n",
    "    comp = pca_1.n_components_\n",
    "    print('No. of components for PCA:' ,comp)\n",
    "    \n",
    "    pca_new = PCA(n_components = comp)\n",
    "    df_pca_new = pca_new.fit_transform(std_df)\n",
    "    \n",
    "    print('Explained variance for ', comp, 'components: ',pca_new.explained_variance_ratio_)\n",
    "    print('Cumulative:', np.cumsum(pca_new.explained_variance_ratio_))\n",
    "    \n",
    "    return(pca_1,df_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145b21c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Model\n",
    "\n",
    "\n",
    "\n",
    "# The value used in the function plays no role as the different hyperparameter value will be used while calling \"create_model\" function\n",
    "def create_model(lyrs=6, neuron_size=64, act='selu', opt='Adam', dr=0.0, learning_rate=0.001,init_weights= 'he_uniform', weight_constraint = 3):\n",
    "    import tensorflow as tf\n",
    "\n",
    "    import keras\n",
    "    from keras.layers import Dense\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dropout\n",
    "    from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # clear model\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    # create first hidden layer\n",
    "    model.add(Dense(neuron_size,input_dim=input_dim, activation=act, kernel_initializer = init_weights, kernel_constraint = max_norm(weight_constraint),kernel_regularizer='l2'))\n",
    "    model.add(Dropout(dr))\n",
    "    #tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    # create additional hidden layers\n",
    "    for i in range(1,lyrs):\n",
    "        model.add(Dense(neuron_size, activation=act, kernel_initializer = init_weights, kernel_constraint = max_norm(weight_constraint),kernel_regularizer='l2'))\n",
    "        model.add(Dropout(dr))\n",
    "        \n",
    "        #tf.keras.layers.BatchNormalization(),\n",
    "    # add dropout, default is none\n",
    "    #model.add(Dropout(dr))\n",
    "    \n",
    "    # create output layer\n",
    "    model.add(Dense(1, activation='ReLU'))  # output layer\n",
    "    opt = Adam(learning_rate=learning_rate)\n",
    "    huber = tf.keras.losses.Huber(delta=1.5)\n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=opt, metrics=['mse', 'mape','mae',tf.keras.metrics.RootMeanSquaredError()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62acbd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrics_plot(history,train_matrics,val_matrics,lable_name,model_name, data_of,plot_path):\n",
    "    import matplotlib.ticker as tck\n",
    "    all_train_mae_histories = []\n",
    "    train_mae_history = train_matrics\n",
    "    all_train_mae_histories.append(train_mae_history)\n",
    "    average_train_mae_history = [\n",
    "        np.mean([x[i] for x in all_train_mae_histories]) for i in range(max_epochs)]\n",
    "\n",
    "    all_val_mae_histories = []\n",
    "    val_mae_history = val_matrics\n",
    "    all_val_mae_histories.append(val_mae_history)\n",
    "    average_val_mae_history = [\n",
    "        np.mean([x[i] for x in all_val_mae_histories]) for i in range(max_epochs)]\n",
    "    \n",
    "    loss = train_matrics\n",
    "    val_loss = val_matrics\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, loss, 'b', linewidth=2, label='Training '+lable_name)\n",
    "    plt.plot(epochs, val_loss, '--r',  linewidth=1, label='Validation '+lable_name)\n",
    "    plt.title('Training and Validation '+lable_name)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(lable_name)\n",
    "    plt.legend()\n",
    "    #plt.savefig('mae_hardness.pdf',dpi=1200)\n",
    "    plt.show()\n",
    "    \n",
    "    def smooth_curve(points, factor=0.9):\n",
    "      smoothed_points = []\n",
    "      for point in points:\n",
    "        if smoothed_points:\n",
    "          previous = smoothed_points[-1]\n",
    "          smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "          smoothed_points.append(point)\n",
    "      return smoothed_points\n",
    "\n",
    "    smooth_train_mae_history = smooth_curve(average_train_mae_history[5:])\n",
    "    smooth_val_mae_history = smooth_curve(average_val_mae_history[5:])\n",
    "    \n",
    "    sns.set(style=\"ticks\", color_codes=True,font_scale=2.25)\n",
    "    fig, ax = plt.subplots(figsize=(6,5.5),dpi=600)\n",
    "    #plt.ylim(20, 120)    # y-label range\n",
    "    plt.gca().yaxis.set_minor_locator(tck.AutoMinorLocator(2))\n",
    "    plt.plot(range(1, len(smooth_train_mae_history) + 1), smooth_train_mae_history, 'b', label = 'Training '+lable_name)\n",
    "    plt.plot(range(1, len(smooth_val_mae_history) + 1),smooth_val_mae_history, '--r', label = 'Validation '+lable_name)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(''+lable_name)\n",
    "    #plt.title('Smooth Training and Validation '+lable_name)\n",
    "    plt.title(data_of+': '+model_name)\n",
    "    plt.legend()\n",
    "    plt.savefig(plot_path+data_of+'_'+lable_name+'.pdf',dpi=1200, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d0547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on test data\n",
    "\n",
    "def r2_plot(model,input_datasets,target_datasets,name,model_name,plot_path=\"plots\\\\hardness\\\\_\"):\n",
    "    sns.set(style=\"ticks\", color_codes=True,font_scale=1.25)\n",
    "    a=0.2 # Percentage error range\n",
    "    predictions_datasets = model.predict(input_datasets)\n",
    "\n",
    "    import sklearn.metrics\n",
    "    from sklearn.metrics import r2_score\n",
    "    r2_test = r2_score(target_datasets, predictions_datasets)\n",
    "    plt.figure(figsize=(4,4.2),dpi=600)\n",
    "\n",
    "    # plot x=y line \n",
    "    x_line = np.linspace(0, 1000, 1000)\n",
    "    \n",
    "    sns.lineplot(x=x_line, y=x_line,color='black',lw=0.75)\n",
    "\n",
    "    print('Test R2 score: ', r2_test)\n",
    "\n",
    "    test_r2 = sns.regplot(x=target_datasets,y=predictions_datasets,ci=None,scatter_kws=dict(s=8,color='r'),fit_reg=False)\n",
    "    test_r2.set(title=str(model_name)+'Performance on Test data,'+' $R^2$ = ' +str(round(r2_test,3)))\n",
    "    test_r2.set_xlabel(\"Real Targets\"+\"(\"+name+\")\", fontsize = 16)\n",
    "    test_r2.set_ylabel(\"Predicted Value\"+\"(\"+name+\")\", fontsize = 16)\n",
    "\n",
    "    Y1 = x_line*(1+a)\n",
    "    Y2 = x_line*(1-a)\n",
    "\n",
    "    sns.lineplot(x=x_line,y=Y1,lw=0.5,color='b',alpha=.2)\n",
    "    sns.lineplot(x=x_line,y=Y2,lw=0.5,color='b',alpha=.2)\n",
    "\n",
    "    test_r2.fill_between(x_line, Y1,x_line,color='b',alpha=.2)\n",
    "    test_r2.fill_between(x_line, Y2,x_line,color='b',alpha=.2)\n",
    "    \n",
    "    # x and y ticks\n",
    "    listOf_Yticks = np.arange(0, 1400, 200)\n",
    "    plt.yticks(listOf_Yticks)\n",
    "    plt.xticks(listOf_Yticks)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ax.yaxis.set_minor_locator(tck.AutoMinorLocator(2))\n",
    "\n",
    "\n",
    "    test_r2.figure.savefig('r2_hardness_'+str(name)+'.png',dpi=1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f182d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def ensemble_model(models, test_datasets,y_test):    \n",
    "    import sklearn.metrics\n",
    "    from sklearn.metrics import r2_score\n",
    "    preds_array=[]\n",
    "    #type(preds_df)\n",
    "    for i in range(len(models)):\n",
    "        preds = models[i].predict(test_datasets[i])\n",
    "        r2_mean = r2_score(y_test,preds)\n",
    "        preds_array.append(preds)\n",
    "\n",
    "        print(\"R sq. for Model\",i,r2_mean)\n",
    "        #print(preds)\n",
    "    preds_array=np.array(preds_array)    \n",
    "    summed = np.sum(preds_array, axis=0)\n",
    "    ensemble_prediction = np.argmax(summed, axis=1)\n",
    "    mean_preds = np.mean(preds_array, axis=0)\n",
    "\n",
    "    r2_mean = r2_score(y_test,mean_preds)\n",
    "    print(\"Average:\",r2_mean )\n",
    "\n",
    "\n",
    "    # Weight calculations\n",
    "    df = pd.DataFrame([])\n",
    "\n",
    "    for w1 in range(0, 4):\n",
    "        for w2 in range(0,4):\n",
    "            for w3 in range(0,4):\n",
    "                for w4 in range(0,4):\n",
    "                    wts = [w1/10.,w2/10.,w3/10.,w4/10.]\n",
    "                    wted_preds1 = np.tensordot(preds_array, wts, axes=((0),(0)))\n",
    "                    wted_ensemble_pred = np.mean(wted_preds1, axis=1)\n",
    "                    weighted_r2 = r2_score(y_test, wted_ensemble_pred)\n",
    "                    df = pd.concat([df,pd.DataFrame({'acc':weighted_r2,'wt1':wts[0],'wt2':wts[1], \n",
    "                                                 'wt3':wts[2],'wt4':wts[3] }, index=[0])], ignore_index=True)\n",
    "\n",
    "    max_r2_row = df.iloc[df['acc'].idxmax()]\n",
    "    print(\"Max $R^2$ of \", max_r2_row[0], \" obained with w1=\", max_r2_row[1],\" w2=\", max_r2_row[2], \" w3=\", max_r2_row[3], \" and w4=\", max_r2_row[4])  \n",
    "    return(preds_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5dfdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "def weighted_ensemble(preds_array, ideal_weights,y_test,plot_path=\"plots\\\\hardness\\\\\", plot_title=\"Hardness Ensemble Model\",limit_value1=1200,limit_value2=1050,value_gap=200):    \n",
    "    import sklearn.metrics\n",
    "    from sklearn.metrics import mean_absolute_percentage_error,mean_absolute_error,mean_squared_error\n",
    "    from sklearn.metrics import r2_score\n",
    "    \n",
    "    import matplotlib.ticker as tck\n",
    "    #Use tensordot to sum the products of all elements over specified axes.\n",
    "\n",
    "    ideal_weighted_preds = np.tensordot(preds_array, ideal_weights, axes=((0),(0)))\n",
    "    ideal_weighted_ensemble_prediction = np.mean(ideal_weighted_preds, axis=1)\n",
    "    \n",
    "    \n",
    "    mae = mean_absolute_error(y_test, ideal_weighted_ensemble_prediction)\n",
    "    mse = mean_squared_error(y_test, ideal_weighted_ensemble_prediction)\n",
    "    rmse = np.sqrt(mse) # or mse**(0.5)  \n",
    "    mape = mean_absolute_percentage_error(y_test, ideal_weighted_ensemble_prediction)\n",
    "    r2 = r2_score(y_test, ideal_weighted_ensemble_prediction)\n",
    "\n",
    "    print(\"Results of sklearn.metrics:\")\n",
    "    print(\"MAE:\",mae)\n",
    "    print(\"MSE:\", mse)\n",
    "    print(\"RMSE:\", rmse)\n",
    "    print(\"MAPE:\", mape)\n",
    "    print(\"R-Squared:\", r2)\n",
    "\n",
    "\n",
    "    #Predict on test data\n",
    "    a=0.2 # Percentage error range\n",
    "    r2_mean = r2_score(y_test,ideal_weighted_ensemble_prediction)\n",
    "    plt.figure(figsize=(6.25,6),dpi=600)\n",
    "\n",
    "    # plot x=y line \n",
    "    x_line = np.linspace(0, limit_value1, 2)\n",
    "    \n",
    "\n",
    "    sns.lineplot(x=x_line, y=x_line,color='black',lw=0.75)\n",
    "\n",
    "    print('Test R2 score: ', r2_mean)\n",
    "\n",
    "    test_r2 = sns.regplot(x=y_test,y=ideal_weighted_ensemble_prediction,ci=None,scatter_kws=dict(s=8,color='b'),fit_reg=False)\n",
    "    test_r2.set_title('$R^2 = $' +str(round(r2_mean,2))+plot_title, fontsize = 24)\n",
    "    test_r2.set_xlabel(\"Real Test Targets\", fontsize = 24)\n",
    "    test_r2.set_ylabel(\"Predicted Test Value\", fontsize = 24)\n",
    "\n",
    "    Y1 = x_line*(1+a)\n",
    "    Y2 = x_line*(1-a)\n",
    "    \n",
    "    \n",
    "\n",
    "    plt.ylim(0, limit_value2)\n",
    "    plt.xlim(0, limit_value2)\n",
    "    test_r2.yaxis.set_minor_locator(tck.AutoMinorLocator(2))\n",
    "    test_r2.xaxis.set_minor_locator(tck.AutoMinorLocator(2))\n",
    "\n",
    "    sns.lineplot(x=x_line,y=Y1,lw=0.5,color='r',alpha=.2)\n",
    "    sns.lineplot(x=x_line,y=Y2,lw=0.5,color='r',alpha=.2)\n",
    "\n",
    "    plt.fill_between(x_line, Y1,x_line,color='r',alpha=.2)\n",
    "    plt.fill_between(x_line, Y2,x_line,color='r',alpha=.2)\n",
    "    test_r2.set(xticks=np.arange(0, limit_value1, value_gap))\n",
    "    test_r2.set(yticks=np.arange(0, limit_value1, value_gap))\n",
    "    test_r2.figure.savefig(plot_path+plot_title+'_r2.png',dpi=1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "691ec75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_model(df,input_pcc,prop=\"Hardness (HV)\", path='hardness_model_files\\\\'):    \n",
    "    input_name = ['$\\delta$', 'Δ$\\chi$', 'ΔTm','Tm(K)', 'VEC', 'AN', 'K', 'B', 'ΔB', 'G', 'ΔG','ΔSmix','$\\lambda$', 'ΔHmix','$\\Omega$']\n",
    "    datasets = df.iloc[:,[-15,-14,-13,-12,-11,-10, -9, -8, -7, -6,-5, -4, -3, -2, -1]]\n",
    "\n",
    "    ##############################################\n",
    "    # One-hot encoding transform from train-model\n",
    "    ###############################################\n",
    "\n",
    "    \n",
    "    import pickle\n",
    "    enc = pickle.load(open(path+'fab_encoding.pkl','rb'))\n",
    "    # Cluster WROUGHT and Unknown into Other\n",
    "    \n",
    "    fab_type = pd.DataFrame(enc.transform(df[['Fabrication_type']]).toarray())\n",
    "\n",
    "    ###################################################\n",
    "    # Input feature Standarization\n",
    "    ################################################\n",
    "    import pickle\n",
    "    scaler = pickle.load(open(path+'scaler.pkl','rb'))\n",
    "\n",
    "    std_input = scaler.transform(datasets)\n",
    "\n",
    "    std_df = pd.DataFrame(data=std_input, columns=input_name)\n",
    "\n",
    "\n",
    "    #########################################################\n",
    "    # Feature Selected\n",
    "    ############################################\n",
    "\n",
    "    # PCC_1\n",
    "\n",
    "    df_pcc_1 = std_df.loc[:,input_pcc]\n",
    "\n",
    "    # PCC_2\n",
    "\n",
    "    inputs_all_fab = np.column_stack((df_pcc_1, fab_type))\n",
    "    df_pcc_2= inputs_all_fab.astype(float)\n",
    "\n",
    "\n",
    "    # PCA-1\n",
    "    pca_1 = pickle.load(open(path+'pca_1.pkl','rb'))\n",
    "    df_pca_1 = pca_1.transform(std_df)\n",
    "    df_pca_1 = pd.DataFrame(df_pca_1)\n",
    "\n",
    "    # PCA-2\n",
    "    inputs_all_fab_pca = np.column_stack((std_df, fab_type))\n",
    "    inputs_all_fab_pca = inputs_all_fab_pca.astype(float)\n",
    "    \n",
    "    pca_2 = pickle.load(open(path+'pca_2.pkl','rb'))\n",
    "    df_pca_2 = pca_2.transform(inputs_all_fab_pca)\n",
    "    df_pca_2 = pd.DataFrame(df_pca_2)\n",
    "\n",
    "\n",
    "    #########################################\n",
    "    # Load ML Model\n",
    "    #########################################\n",
    "    from keras.models import load_model\n",
    "\n",
    "    load_model_pcc_1 = load_model(path+'best_model_pcc_1.h5')\n",
    "    \n",
    "    load_model_pcc_2 = load_model(path+'best_model_pcc_2.h5')\n",
    "    load_model_pca_1 = load_model(path+'best_model_pca_1.h5')\n",
    "    load_model_pca_2 = load_model(path+'best_model_pca_2.h5')\n",
    "\n",
    "\n",
    "    ########################################\n",
    "    # Ensemble Model\n",
    "    #######################################\n",
    "    models = [load_model_pcc_1,load_model_pcc_2, load_model_pca_1, load_model_pca_2]\n",
    "    test_datasets = [df_pcc_1,df_pcc_2, df_pca_1, df_pca_2]\n",
    "\n",
    "    ideal_weights = [0.1,0.3, 0.3, 0.3] \n",
    "    \n",
    "    \n",
    " \n",
    "    #preds_array = ensemble_model(load_models, test_datasets, y_test)\n",
    "    #weighted_ensemble(preds_array, ideal_weights,y_test,plot_path=\"plots\\\\hardness\\\\\", plot_title=\"Hardness Ensemble Model\",limit_value1=1200,limit_value2=1050,value_gap=200)\n",
    "    \n",
    "#    '''    \n",
    "    y_test = df[prop]  \n",
    "    import sklearn.metrics\n",
    "    from sklearn.metrics import r2_score\n",
    "    preds_array=[]\n",
    " \n",
    "    #type(preds_df)\n",
    "    for i in range(len(models)):\n",
    "        preds = models[i].predict(test_datasets[i])\n",
    "        #r2_mean = r2_score(y_test,preds)\n",
    "        preds_array.append(preds)\n",
    "\n",
    "        #print(i,r2_mean)\n",
    "        #print(preds)\n",
    "    preds_array=np.array(preds_array)    \n",
    "    summed = np.sum(preds_array, axis=0)\n",
    "    ensemble_prediction = np.argmax(summed, axis=1)\n",
    "    mean_preds = np.mean(preds_array, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    # Weight calculations\n",
    "    df = pd.DataFrame([])\n",
    "\n",
    "    for w1 in range(0, 4):\n",
    "        for w2 in range(0,4):\n",
    "            for w3 in range(0,4):\n",
    "                for w4 in range(0,4):\n",
    "                    wts = [w1/10.,w2/10.,w3/10.,w4/10.]\n",
    "                    wted_preds1 = np.tensordot(preds_array, wts, axes=((0),(0)))\n",
    "                    wted_ensemble_pred = np.mean(wted_preds1, axis=1)\n",
    "                    weighted_r2 = r2_score(y_test, wted_ensemble_pred)\n",
    "                    df = pd.concat([df,pd.DataFrame({'acc':weighted_r2,'wt1':wts[0],'wt2':wts[1], \n",
    "                                                 'wt3':wts[2],'wt4':wts[3] }, index=[0])], ignore_index=True)\n",
    "\n",
    "    max_r2_row = df.iloc[df['acc'].idxmax()]\n",
    "    print(\"Max $R^2$ of \", max_r2_row[0], \" obained with w1=\", max_r2_row[1],\" w2=\", max_r2_row[2], \" w3=\", max_r2_row[3], \" and w4=\", max_r2_row[4]) \n",
    "#    '''    \n",
    " \n",
    "    \n",
    "    #Use tensordot to sum the products of all elements over specified axes.\n",
    "    ideal_weighted_preds = np.tensordot(preds_array, ideal_weights, axes=((0),(0)))\n",
    "    ideal_weighted_ensemble_prediction = np.mean(ideal_weighted_preds, axis=1)\n",
    "    \n",
    "    return ideal_weighted_ensemble_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e61155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_model_new(df,input_pcc,ideal_weights, path='hardness_model_files\\\\'):    \n",
    "    input_name = ['$\\delta$', 'Δ$\\chi$', 'ΔTm','Tm(K)', 'VEC', 'AN', 'K', 'B', 'ΔB', 'G', 'ΔG','ΔSmix','$\\lambda$', 'ΔHmix','$\\Omega$']\n",
    "    datasets = df.iloc[:,[-15,-14,-13,-12,-11,-10, -9, -8, -7, -6,-5, -4, -3, -2, -1]]\n",
    "\n",
    "    ##############################################\n",
    "    # One-hot encoding transform from train-model\n",
    "    ###############################################\n",
    "\n",
    "    \n",
    "    import pickle\n",
    "    enc = pickle.load(open(path+'fab_encoding.pkl','rb'))\n",
    "    # Cluster WROUGHT and Unknown into Other\n",
    "    \n",
    "    fab_type = pd.DataFrame(enc.transform(df[['Fabrication_type']]).toarray())\n",
    "\n",
    "    ###################################################\n",
    "    # Input feature Standarization\n",
    "    ################################################\n",
    "    import pickle\n",
    "    scaler = pickle.load(open(path+'scaler.pkl','rb'))\n",
    "\n",
    "    std_input = scaler.transform(datasets)\n",
    "\n",
    "    std_df = pd.DataFrame(data=std_input, columns=input_name)\n",
    "\n",
    "\n",
    "    #########################################################\n",
    "    # Feature Selected\n",
    "    ############################################\n",
    "\n",
    "    # PCC_1\n",
    "    #input_pcc = ['$\\delta$', 'Δ$\\chi$', 'ΔTm', 'VEC', '$\\lambda$', 'ΔHmix']\n",
    "    df_pcc_1 = std_df.loc[:,input_pcc]\n",
    "\n",
    "    # PCC_2\n",
    "\n",
    "    inputs_all_fab = np.column_stack((df_pcc_1, fab_type))\n",
    "    df_pcc_2= inputs_all_fab.astype(float)\n",
    "\n",
    "\n",
    "    # PCA-1\n",
    "    pca_1 = pickle.load(open(path+'pca_1.pkl','rb'))\n",
    "    df_pca_1 = pca_1.transform(std_df)\n",
    "    df_pca_1 = pd.DataFrame(df_pca_1)\n",
    "\n",
    "    # PCA-2\n",
    "    inputs_all_fab_pca = np.column_stack((std_df, fab_type))\n",
    "    inputs_all_fab_pca = inputs_all_fab_pca.astype(float)\n",
    "    \n",
    "    pca_2 = pickle.load(open(path+'pca_2.pkl','rb'))\n",
    "    df_pca_2 = pca_2.transform(inputs_all_fab_pca)\n",
    "    df_pca_2 = pd.DataFrame(df_pca_2)\n",
    "\n",
    "\n",
    "    #########################################\n",
    "    # Load ML Model\n",
    "    #########################################\n",
    "    from keras.models import load_model\n",
    "\n",
    "    load_model_pcc_1 = load_model(path+'best_model_pcc_1.h5')\n",
    "    \n",
    "    load_model_pcc_2 = load_model(path+'best_model_pcc_2.h5')\n",
    "    load_model_pca_1 = load_model(path+'best_model_pca_1.h5')\n",
    "    load_model_pca_2 = load_model(path+'best_model_pca_2.h5')\n",
    "\n",
    "\n",
    "    ########################################\n",
    "    # Ensemble Model\n",
    "    #######################################\n",
    "    models = [load_model_pcc_1,load_model_pcc_2, load_model_pca_1, load_model_pca_2]\n",
    "    test_datasets = [df_pcc_1,df_pcc_2, df_pca_1, df_pca_2]\n",
    "\n",
    "    #ideal_weights = [0.3,0.3, 0.1, 0.3] \n",
    "    \n",
    "    \n",
    " \n",
    "    #preds_array = ensemble_model(load_models, test_datasets, y_test)\n",
    "    #weighted_ensemble(preds_array, ideal_weights,y_test,plot_path=\"plots\\\\hardness\\\\\", plot_title=\"Hardness Ensemble Model\",limit_value1=1200,limit_value2=1050,value_gap=200)\n",
    "    \n",
    "\n",
    "    import sklearn.metrics\n",
    "    from sklearn.metrics import r2_score\n",
    "    preds_array=[]\n",
    " \n",
    "    #type(preds_df)\n",
    "    for i in range(len(models)):\n",
    "        preds = models[i].predict(test_datasets[i])\n",
    "        #r2_mean = r2_score(y_test,preds)\n",
    "        preds_array.append(preds)\n",
    "\n",
    "        #print(i,r2_mean)\n",
    "        #print(preds)\n",
    "    preds_array=np.array(preds_array)    \n",
    "    summed = np.sum(preds_array, axis=0)\n",
    "    ensemble_prediction = np.argmax(summed, axis=1)\n",
    "    mean_preds = np.mean(preds_array, axis=0)\n",
    "    \n",
    "\n",
    "    '''    \n",
    "    y_test = df[\"Hardness (HV)\"]  \n",
    "\n",
    "    # Weight calculations\n",
    "    df = pd.DataFrame([])\n",
    "\n",
    "    for w1 in range(0, 4):\n",
    "        for w2 in range(0,4):\n",
    "            for w3 in range(0,4):\n",
    "                for w4 in range(0,4):\n",
    "                    wts = [w1/10.,w2/10.,w3/10.,w4/10.]\n",
    "                    wted_preds1 = np.tensordot(preds_array, wts, axes=((0),(0)))\n",
    "                    wted_ensemble_pred = np.mean(wted_preds1, axis=1)\n",
    "                    weighted_r2 = r2_score(y_test, wted_ensemble_pred)\n",
    "                    df = pd.concat([df,pd.DataFrame({'acc':weighted_r2,'wt1':wts[0],'wt2':wts[1], \n",
    "                                                 'wt3':wts[2],'wt4':wts[3] }, index=[0])], ignore_index=True)\n",
    "\n",
    "    max_r2_row = df.iloc[df['acc'].idxmax()]\n",
    "    print(\"Max $R^2$ of \", max_r2_row[0], \" obained with w1=\", max_r2_row[1],\" w2=\", max_r2_row[2], \" w3=\", max_r2_row[3], \" and w4=\", max_r2_row[4]) \n",
    "    '''    \n",
    " \n",
    "    \n",
    "    #Use tensordot to sum the products of all elements over specified axes.\n",
    "    ideal_weighted_preds = np.tensordot(preds_array, ideal_weights, axes=((0),(0)))\n",
    "    ideal_weighted_ensemble_prediction = np.mean(ideal_weighted_preds, axis=1)\n",
    "    \n",
    "    return ideal_weighted_ensemble_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cc56b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ternary_plot(df,input_pcc,ideal_weights,element1,element2,fab_cat=\"CAT-A\", pole_labels=['Al','Ti', '(CrFeNi)'],baxis_min=0.9, model_path='hardness_model_files\\\\'):\n",
    "    df = df.set_axis(['Alloys'], axis=1, inplace=False)\n",
    "\n",
    "    df[\"Fabrication_type\"]=fab_cat\n",
    "    \n",
    "    df = featurization(df)\n",
    "\n",
    "\n",
    "    df_prop, df_input_target = properties_calculation(df)\n",
    "\n",
    "    #print(df_plot.head())\n",
    "    predicted_value =prediction_model_new(df_prop,input_pcc,ideal_weights, path=model_path)\n",
    "    df_result_ternary = pd.DataFrame(predicted_value)\n",
    "    \n",
    "    import plotly.figure_factory as ff\n",
    "    import numpy as np\n",
    "\n",
    "    #Al = np.array([0,0,0,0,0,0,0,0.1,0.2, 0.3, 0.4, 0.5, 0.6, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "    #Ti = np.array([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0, 0, 0, 0,0,0,0.1, 0.2, 0.3, 0.4, 0.5, 0.1, 0.1, 0.1, 0.1])\n",
    "    comp = 1 - element1 - element2\n",
    "\n",
    "    print(predicted_value.shape,element1.shape,comp.shape)\n",
    "\n",
    "    fig = ff.create_ternary_contour(np.array([comp,element1,element2]), predicted_value,\n",
    "                                    pole_labels=pole_labels,\n",
    "                                    interp_mode='cartesian',\n",
    "                                    ncontours=20,\n",
    "                                    colorscale='Viridis',\n",
    "                                    showscale=True,\n",
    "                                    width=700, height=500)\n",
    "\n",
    "    fig.update_ternaries(baxis_nticks=10)\n",
    "    fig.update_ternaries(aaxis_nticks=10)\n",
    "    fig.update_ternaries(caxis_nticks=10)\n",
    "\n",
    "    fig.update_layout(\n",
    "        #title=title,\n",
    "        title_font_size=20,\n",
    "        xaxis_title=\"X Axis Title\",\n",
    "        yaxis_title=\"Y Axis Title\",\n",
    "        legend_title=\"Legend Title\",\n",
    "        #value_title = \"a\"\n",
    "        font=dict(\n",
    "            #family=\"Courier New, monospace\",\n",
    "            size=24,\n",
    "            color=\"black\"\n",
    "        )\n",
    "    )\n",
    "    fig.update_ternaries(sum=1, baxis_min=baxis_min);\n",
    "\n",
    "\n",
    "\n",
    "    # Axis labels. (See below for corner labels.)\n",
    "    fontsize = 24\n",
    "    offset = 0.08\n",
    "\n",
    "    plot_fig_loc = ('plots\\\\predictions\\\\Test_ternary_plot.pdf')\n",
    "\n",
    "    #plt.savefig('plots\\\\predictions\\\\Test_ternary_plot.pdf')\n",
    "    fig.show()\n",
    "\n",
    "    return predicted_value\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
